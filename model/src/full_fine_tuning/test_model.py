import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import json
import os
import pickle

def load_model():
    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    model_path = os.path.abspath("./mbti-style-transfer-model")
    print(f"ëª¨ë¸ ê²½ë¡œ: {model_path}")
    
    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    return model, tokenizer

def translate_text(model, tokenizer, text, target_mbti):
    # ì…ë ¥ í…ìŠ¤íŠ¸ ì¤€ë¹„ (í•™ìŠµ ë°ì´í„° í˜•ì‹ê³¼ ì¼ì¹˜í•˜ë„ë¡)
    input_text = f"{target_mbti.lower()} ë§íˆ¬ë¡œ ë³€í™˜:{text}"
    inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True).to(model.device)
    
    inputs = {k: v for k, v in inputs.items() if k != "token_type_ids"}

    # # ğŸ‘‡ ë””ë²„ê¹…ìš© ë¡œê·¸ ì¶”ê°€
    # print(f"[ë””ë²„ê¹…] input_text: {input_text}")
    # print(f"[ë””ë²„ê¹…] input_ids: {inputs['input_ids']}")

    # ë²ˆì—­ ìƒì„±
    outputs = model.generate(
        **inputs,
        max_new_tokens=64,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.2,
        early_stopping=True
    )
    
    # ê²°ê³¼ ë””ì½”ë”©
    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return translated_text

def test_model():
    # í…ŒìŠ¤íŠ¸í•  ë¬¸ì¥ë“¤
    test_sentences = [
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”.",
        "ë‚˜ë‘ ì˜í™” ë³¼ë˜?",
        "ì´ë²ˆ ì‹œí—˜ ì˜ ë´¤ì–´?",
        "ì·¨ë¯¸ê°€ ë­ì•¼?",
        "ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì„ ë•Œ ì–´ë–»ê²Œ í•´?",
        "ì¢‹ì•„í•˜ëŠ” ìŒì‹ì´ ë­ì•¼?"
        # "ì˜¤ëŠ˜ ì ì‹¬ ë­ ë¨¹ì—ˆì–´?",
        # "ì£¼ë§ì— ë­í•˜ì‹¤ ê³„íšì´ì—ìš”?"
    ]
    
    # í…ŒìŠ¤íŠ¸í•  MBTI ìœ í˜•ë“¤ (ì†Œë¬¸ìë¡œ ë³€ê²½)
    mbtis = [
            "intj", "intp", "entj", "entp", "infj", "infp", "enfj", "enfp",
            "istj", "istp", "estj", "estp", "isfj", "isfp", "esfj", "esfp"
        ]
    mbti_types = [mbti.lower() for mbti in mbtis]
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_model()
    
    print("\n=== MBTI ìŠ¤íƒ€ì¼ ë³€í™˜ í…ŒìŠ¤íŠ¸ ===")
    print("ì›ë³¸ ë¬¸ì¥ -> MBTI ìœ í˜•ë³„ ë³€í™˜ ê²°ê³¼\n")
    
    for sentence in test_sentences:
        print(f"\nì›ë³¸: {sentence}")
        print("-" * 50)
        for mbti in mbti_types:
            translated = translate_text(model, tokenizer, sentence, mbti)
            print(f"{mbti}: {translated}")
        print("-" * 50)

if __name__ == "__main__":
    test_model() 