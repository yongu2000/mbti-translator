import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import json
import os
import pickle

def load_model():
    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    model_path = os.path.abspath("model/checkpoints/mbti_style")
    # current_dir = os.path.dirname(os.path.abspath(__file__))
    # model_path = os.path.join(os.path.dirname(current_dir), "checkpoints", "mbti_style")
    
    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_path, local_files_only=True)

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    return model, tokenizer

def translate_text(model, tokenizer, text, target_mbti):
    # ì…ë ¥ í…ìŠ¤íŠ¸ ì¤€ë¹„ (í•™ìŠµ ë°ì´í„° í˜•ì‹ê³¼ ì¼ì¹˜í•˜ë„ë¡)
    input_text = f"{target_mbti} ë§íˆ¬ë¡œ ë³€í™˜:{text}"
    inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True).to(model.device)
    
    inputs = {k: v for k, v in inputs.items() if k != "token_type_ids"}

    # # ğŸ‘‡ ë””ë²„ê¹…ìš© ë¡œê·¸ ì¶”ê°€
    # print(f"[ë””ë²„ê¹…] input_text: {input_text}")
    # print(f"[ë””ë²„ê¹…] input_ids: {inputs['input_ids']}")

    # ë²ˆì—­ ìƒì„±
    outputs = model.generate(
        **inputs,
        max_new_tokens=64,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.2,
        early_stopping=True
    )
    
    # ê²°ê³¼ ë””ì½”ë”©
    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return translated_text

def test_model():
    # í…ŒìŠ¤íŠ¸í•  ë¬¸ì¥ë“¤
    test_sentences = [
        "ë”ìš°ë‹ˆê¹Œ ìˆ˜ë°• ë¨¹ê³ ì‹¶ë‹¤",
        "ë„ˆëŠ” ì™œ ë‚˜í•œí…Œ ê·¸ë ‡ê²Œ í™”ë§Œ ë‚´?",
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”.",
        "ì´ë²ˆ ì£¼ë§ì— ë­í•˜ì‹¤ ê³„íšì´ì—ìš”?",
        "ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•˜ê²Œ ë˜ì—ˆì–´ìš”.",
        "ì ì‹¬ ë­ ë¨¹ì—ˆì–´?",
        "ìš”ì¦˜ ì–´ë–»ê²Œ ì§€ë‚´?",
        "ë‚˜ë‘ ì˜í™” ë³¼ë˜?",
        "ì´ë²ˆ ì‹œí—˜ ì˜ ë´¤ì–´?",
        "ì·¨ë¯¸ê°€ ë­ì•¼?",
        "ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì„ ë•Œ ì–´ë–»ê²Œ í•´?",
        "ì¢‹ì•„í•˜ëŠ” ìŒì‹ì´ ë­ì•¼?"
        # "ì˜¤ëŠ˜ ì ì‹¬ ë­ ë¨¹ì—ˆì–´?",
        # "ì£¼ë§ì— ë­í•˜ì‹¤ ê³„íšì´ì—ìš”?"
    ]
    
    # í…ŒìŠ¤íŠ¸í•  MBTI ìœ í˜•ë“¤
    mbti_types = ["ISTP", "ISFJ"]
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_model()
    
    print("\n=== MBTI ìŠ¤íƒ€ì¼ ë³€í™˜ í…ŒìŠ¤íŠ¸ ===")
    print("ì›ë³¸ ë¬¸ì¥ -> MBTI ìœ í˜•ë³„ ë³€í™˜ ê²°ê³¼\n")
    
    for sentence in test_sentences:
        print(f"\nì›ë³¸: {sentence}")
        print("-" * 50)
        for mbti in mbti_types:
            translated = translate_text(model, tokenizer, sentence, mbti)
            print(f"{mbti}: {translated}")
        print("-" * 50)

if __name__ == "__main__":
    test_model() 